{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import itertools\n",
    "import pickle\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import dedupe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "\n",
    "from io import StringIO\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_file_name = 'configuration_file_dedupe.json'\n",
    "config_file_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configuration_file_name, 'r') as config_file:\n",
    "    config_file_data = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "input_file_1 = config_file_data.get('input_file_1')\n",
    "input_file_2 = config_file_data.get('input_file_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    This function does a little bit of data cleaning with the help of Unidecode and Regex libraries.\n",
    "    Things like casing, extra spaces and new lines can be ignored.\n",
    "    \n",
    "    :param column: string object which represents a cell from the csv file\n",
    "    :return: the preprocessed column\n",
    "    \"\"\"\n",
    "    \n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    \n",
    "    if not column :\n",
    "        column = None\n",
    "        \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    This function reads CSV file and creates a dictionary of records, \n",
    "    where the key is a unique record ID (name of the file + index).\n",
    "    \n",
    "    :param filename: string object which represents the name of the\n",
    "                     input file\n",
    "    :return: a dictionary object containing all the rows read from the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    \n",
    "    partial_key = filename\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        \n",
    "        # delete the first 4 characters of the partial key if they are \"tmp_\"\n",
    "        if partial_key.find(\"tmp_\") == 0:\n",
    "            partial_key = partial_key[4:]\n",
    "            \n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k,v) in row.items()])\n",
    "            data_d[partial_key + str(i)] = dict(clean_row)\n",
    "            \n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_containing_only_the_common_fields_in_both_datasets(input_file, common_fields):\n",
    "    \"\"\"\n",
    "    This function reads from a CSV file only the common fields of both datasets.\n",
    "    \n",
    "    :param input_file: string object which represents the name of the\n",
    "                       input file\n",
    "    :param common_fields: a list of string objects containing the name of the \n",
    "                          common fields of the both datasets\n",
    "    :return: a dictionary object contai\n",
    "    \"\"\"\n",
    "\n",
    "    # create a temporary file with only common columns\n",
    "    tmp_input_file_with_common_cols = \"tmp_\" + input_file\n",
    "\n",
    "    df = pd.read_csv(input_file, dtype=object)\n",
    "    df = df[common_fields]\n",
    "\n",
    "    df.to_csv(tmp_input_file_with_common_cols, index=False)\n",
    "\n",
    "    # read the data from this temporary file\n",
    "    data_d = read_data(tmp_input_file_with_common_cols)\n",
    "\n",
    "    # remove the temporary file \n",
    "    os.remove(tmp_input_file_with_common_cols)\n",
    "\n",
    "    return data_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_fields_of_the_datasets_or_the_given_fields(config_file_data, input_file_1, input_file_2):\n",
    "    \"\"\"\n",
    "    Get the common columns (fields) of both datasets if the user did not specify \n",
    "    them in the configuration file.\n",
    "    \n",
    "    :param config_file_data: a dictionary containing the information of the\n",
    "                            configuration file\n",
    "    :param input_file_1: string object which represents the name of the first dataset\n",
    "    :param input_file_2: string object which represents the name of the second dataset\n",
    "    :return: a list containing the common columns of both datasets\n",
    "    \"\"\"\n",
    "    given_fields = []\n",
    "    \n",
    "    # if some fields were specified, get them in a list\n",
    "    if config_file_data['training'].get('field_definitions'):\n",
    "        field_definitions = config_file_data['training'].get('field_definitions')\n",
    "        \n",
    "        for f in field_definitions:\n",
    "            given_fields.append(f['field'])  \n",
    "        \n",
    "    # get the common columns of both datasets\n",
    "    df1 = pd.read_csv(input_file_1, dtype = object)\n",
    "    df2 = pd.read_csv(input_file_2, dtype = object)\n",
    "      \n",
    "    # postgres only has lower case column names --> make lower case the dataframe column names    \n",
    "    f1_header_columns = set([x.lower() for x in list(df1.columns.values)])\n",
    "    f2_header_columns = set([x.lower() for x in list(df2.columns.values)])\n",
    "    \n",
    "    common_cols_from_datasets = list(f1_header_columns.intersection(f2_header_columns))\n",
    "    \n",
    "    # make sure that the given fields (if given) can be found in the common columns of the datasets\n",
    "    if len(given_fields) > 0:\n",
    "        return list(set(given_fields).intersection(set(common_cols_from_datasets)))\n",
    "    \n",
    "    return common_cols_from_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = get_common_fields_of_the_datasets_or_the_given_fields(config_file_data, input_file_1, input_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('reading records from {}'.format(input_file_1))\n",
    "first_dataset = read_dataset_containing_only_the_common_fields_in_both_datasets(input_file_1, common_columns)\n",
    "logging.info('{} records read'.format(len(first_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('reading records from {}'.format(input_file_2))\n",
    "second_dataset = read_dataset_containing_only_the_common_fields_in_both_datasets(input_file_2, common_columns)\n",
    "logging.info('{} records read'.format(len(second_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('starting training..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_fields(config_file_data, common_columns):\n",
    "    \"\"\"\n",
    "    Define the common columns (fields) the library will pay attention to \n",
    "    by creating a list of dictionaries where each dictionary's keys will be 'field' and\n",
    "    'type'.\n",
    "    E.g.: [{'field' : 'field1', 'type' : 'String'}, {'field' : 'field2', 'type' : 'String'}]\n",
    "    For more information about the accepted types, read more here:\n",
    "    https://docs.dedupe.io/en/latest//Variable-definition.html\n",
    "    \n",
    "    :param config_file_data: dictionary containg the the information of the\n",
    "                             configuration file\n",
    "    :param common_columns: a list of string objects containing the name of the \n",
    "                           common columns(fields) of the both datasets\n",
    "    :return: a list containing dictionaries which represents the fields used for training\n",
    "    \"\"\"\n",
    "    if config_file_data['training'].get('field_definitions'):\n",
    "        fields = []\n",
    "        for c in common_columns:\n",
    "            for f in config_file_data['training']['field_definitions']:\n",
    "                if f['field'] == c:\n",
    "                    fields.append({'field' : c, 'type' : f['type']})\n",
    "                    break\n",
    "                    \n",
    "        return fields\n",
    "    \n",
    "    #if the fields are not specified in configuration file, then create\n",
    "    #a list of dictionaries where the type of the field will be \"String\" \n",
    "    #by default\n",
    "    fields = [{'field': f, 'type': 'String'} for f in common_columns]\n",
    "    \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertain_pairs(deduper, nr_uncertain_pairs):\n",
    "    \"\"\"\n",
    "    This function gets all the uncertain pairs by using the library's function uncertainPairs.\n",
    "    \n",
    "    :param deduper: the library object\n",
    "    :param nr_uncertain_pairs: how many uncertain pairs the library should give to the user to label\n",
    "    :return: a list of tuples where every tuple represents a pair of examples which library is uncertain to label them.\n",
    "    \"\"\"\n",
    "    \n",
    "    uncertain_pairs = [] \n",
    "    \n",
    "    for i in range(0, nr_uncertain_pairs):\n",
    "        uncertain_pair = deduper.uncertainPairs()\n",
    "        uncertain_pairs.append(uncertain_pair.pop())\n",
    "    \n",
    "    return uncertain_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the algorithm is the training part. In this part the [RecordLink](https://docs.dedupe.io/en/latest/API-documentation.html#RecordLink) class from Dedupe is used. This class was chosen because the algorithm takes two input files and tries to find a one-to-one match between different examples from the two input data sets.\n",
    "A RecordLink object is created using the common fields of the two input data sets. If the user specifies these fields in the configuration file, then, the ones that are common to both data sets will be used (they may be just a subset of all the common columns that exist in the input files). These fields are used as features for the classifier. In this part the algorithm will learn different weights for these fields and then it will build a model. Basically, the model is represented by the the fields and their weights.\n",
    "Regarding the training file, we have three cases:\n",
    "* In the first case if the training and settings file don't exist, the user should create them. First the algorithm samples some examples to do a 'mini training', that will be used for giving, to the user, the examples that the library is uncertain about. The user should label this examples by typing _y_(yes) in case the user considers that these examples match, _n_(no) in case the examples don't match, _u_(unsure) if the user is unsure about the examples and how to label them, _p_(previous) if the user figures out that he/she didn't label correctly the previous example or _f_(finish) to finish the labeling part. At the end of this process, which is called **active learning**, a training file will be created and the library will use it to do the actual training. The result is a model which will be saved in a settings file. For building the model, the library uses [Regularized Logistic Regression](https://github.com/dedupeio/rlr) classifier.\n",
    "* In the second case if a training file exists, then the labeling part is skipped and a new model is created/learnt.\n",
    "* In the third case if a settings file exists, the labeling part and training part is skipped, and the model is read from the settings file.\n",
    "\n",
    "**Note**: the user can change the classifier that Dedupe uses at this part; more info about this [here](https://docs.dedupe.io/en/latest/API-documentation.html#Dedupe.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if 'create_training_file_by_client' parameter is true this cell will create the 'uncertain_pairs_file'\n",
    "# configuration parameters used for the training part\n",
    "nr_examples_sampled_for_training = config_file_data['training'].get('nr_of_examples_for_training')\n",
    "training_fields = get_training_fields(config_file_data, common_columns)\n",
    "create_training_file = config_file_data['training']['create_training_file_by_client']\n",
    "training_file = config_file_data['training'].get('training_file')\n",
    "settings_file = config_file_data['training'].get('settings_file')\n",
    "\n",
    "if settings_file:\n",
    "    logging.info('reading from {}'.format(settings_file))\n",
    "    with open(settings_file, 'rb') as sf :\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "        \n",
    "else:\n",
    "    linker = dedupe.RecordLink(training_fields)\n",
    "\n",
    "    linker.sample(first_dataset, second_dataset, nr_examples_sampled_for_training)\n",
    "     \n",
    "    # if the user wants to create a training file on the client side we will make available\n",
    "    # through a GET request a binary file containing 200 pairs of examples \n",
    "    # that Dedupe doesn't know if they match or not; then, on the client side\n",
    "    # the user will label the pairs (match or distinct) and the client will\n",
    "    # make a POST request with the newly created training file\n",
    "    if create_training_file:\n",
    "        uncertain_pairs = get_uncertain_pairs(linker, 200)\n",
    "        with open(\"uncertain_pairs_file\", \"wb\") as f:\n",
    "            pickle.dump(uncertain_pairs, f)\n",
    "\n",
    "    else:     \n",
    "        if training_file:\n",
    "            logging.info('reading labeled examples from {}'.format(training_file))\n",
    "            with open(training_file) as tf :\n",
    "                linker.readTraining(tf)\n",
    "        else:    \n",
    "            logging.info('starting active labeling...')\n",
    "            dedupe.consoleLabel(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        # if the training and settings files were not specified, but you want to keep them between runs, \n",
    "        # rename them or save them somewhere else, because they will be overwritten every time the algorithm is run\n",
    "        if not training_file:\n",
    "            with open(\"training_file.json\", 'w') as tf:\n",
    "                linker.writeTraining(tf)\n",
    "\n",
    "        if not settings_file:\n",
    "            with open(\"settings_file\", 'wb') as sf:\n",
    "                linker.writeSettings(sf)\n",
    "\n",
    "        linker.cleanupTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings_file is None and create_training_file:\n",
    "    logging.info('reading labeled examples from the newly created training file {}'.format(training_file))\n",
    "    \n",
    "    with open(training_file) as tf:\n",
    "        linker.readTraining(tf)\n",
    "\n",
    "    linker.train()\n",
    "    \n",
    "    with open(\"settings_file\", 'wb') as sf:\n",
    "        linker.writeSettings(sf)\n",
    "\n",
    "    linker.cleanupTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the algorithm consists of creating clusters. Before, a threshold value should be calculated using the [threshold](https://docs.dedupe.io/en/latest/API-documentation.html#RecordLink.threshold) method from Dedupe library. Threshold is a float number between 0 and 1. The records will be considered as potential duplicates if the predicted probability of being duplicates is above this threshold. Lowering the threshold number will increase recall, raising it will increase precision. For reducing the time of computing the threshold, a given number of examples are sampled from both datasets. \n",
    "\n",
    "Based on the parameters given in the configuration fie, we have two cases:\n",
    "* The threshold value is given by the user and then the computing part is skipped.\n",
    "* In case the threshold value is not given, the user should specify, in the configuration file, the _recall_weight_ parameter and the _nr_of_sample_data_for_threshold_ parameter. _Recall_ weight is used for computing the threshold and _nr_of_sample_data_for_threshold_ is used for getting the samples of examples from the input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters used for the 'threshold' method from Dedupe\n",
    "threshold_value = None\n",
    "\n",
    "if config_file_data.get('threshold') or config_file_data.get('compute_threshold'):\n",
    "    if config_file_data.get('threshold'):\n",
    "        threshold_value = config_file_data.get('threshold')\n",
    "    else:\n",
    "        logging.info('find the best threshold...')\n",
    "\n",
    "        recall_weight = config_file_data['compute_threshold'].get('recall_weight')\n",
    "\n",
    "        sample_nr_of_examples_for_threshold = \\\n",
    "            config_file_data['compute_threshold'].get('nr_of_sample_data_for_threshold')\n",
    "\n",
    "        # get n examples from the input dataset, where n = 'sample_nr_of_examples_for_threshold' \n",
    "        sample_data_threshold_first_dataset = {\n",
    "            k: first_dataset[k] for k in list(first_dataset)[:int(sample_nr_of_examples_for_threshold)]\n",
    "        }\n",
    "        sample_data_threshold_second_dataset = {\n",
    "            k: second_dataset[k] for k in list(second_dataset)[:int(sample_nr_of_examples_for_threshold)]\n",
    "        }\n",
    "\n",
    "        threshold_value = linker.threshold(sample_data_threshold_first_dataset,\n",
    "                                           sample_data_threshold_second_dataset,\n",
    "                                           recall_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the clustering part the [match](https://docs.dedupe.io/en/latest/API-documentation.html#RecordLink.match) method from Dedupe library is used. This method returns a list which contains tuples of record ids, of the input examples that match, and also their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info('clustering...')\n",
    "print(threshold_value)\n",
    "\n",
    "if threshold_value:\n",
    "    linked_records = linker.match(first_dataset, second_dataset, threshold_value)\n",
    "else:\n",
    "    linked_records = linker.match(first_dataset, second_dataset)\n",
    "\n",
    "logging.info('# duplicate sets {}'.format(len(linked_records)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maximum value for cluster_id found in the database\n",
    "# we will create new clusters, which will have ids starting from this value onward\n",
    "cluster_id = int(config_file_data.get('last_cluster_id')) if config_file_data.get('last_cluster_id') else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a dictionary where the keys will be the record id given\n",
    "# in the read_data method and the values will be tuples which\n",
    "# contain the cluster id and the score. The output files are created using this\n",
    "# dictionary\n",
    "\n",
    "cluster_membership = {}\n",
    "\n",
    "for cluster, score in linked_records:\n",
    "    cluster_id += 1    \n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score) \n",
    "\n",
    "unique_id = cluster_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part creates two output files based on the input files, but adding two more columns: _cluster_id_ and _link_score_.\n",
    "\n",
    "**Cluster_id** column represents a number which is assigned to two examples that match. A _cluster_id_ will also be assigned to one example from one dataset, in case it doesn't have any matches in the other dataset.\n",
    "\n",
    "**Link_score** column represents a measurement of how similar two examples from the same cluster are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_file(filename, output_file):\n",
    "    \"\"\"\n",
    "    Create an output file which contains cluster id and the link score columns\n",
    "    besides the initial columns from the input file \n",
    "     \n",
    "    :param filename: string object which represents the name of one dataset\n",
    "    :param output_file: string object which represents the name of the output\n",
    "                       file\n",
    "    \"\"\"\n",
    "\n",
    "    global unique_id\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        with open(filename) as f_input:\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            heading_row = next(reader)\n",
    "            heading_row.insert(0, 'link_score')\n",
    "            heading_row.insert(0, 'cluster_id')\n",
    "            writer.writerow(heading_row)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "\n",
    "                # the examples which have not a match with other examples will be put\n",
    "                # in their own cluster\n",
    "                if cluster_details is None:\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else:\n",
    "                    cluster_id, score = cluster_details\n",
    "\n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"output_\" + input_file_1\n",
    "output_file_2 = \"output_\" + input_file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('create output files...')\n",
    "create_output_file(input_file_1, output_file_1)\n",
    "create_output_file(input_file_2, output_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the clusters made by library are evaluated. We want to see how good the algorithm performed. For evaluation, [precision](https://en.wikipedia.org/wiki/Precision_and_recall) and [recall](https://en.wikipedia.org/wiki/Precision_and_recall) are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('starting evaluation...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_dataframe_containing_only_cluster_id_and_label_column(output_file_1, output_file_2, columns):\n",
    "    \"\"\"\n",
    "    Create a merged dataframe which contains only the cluster id and label(\"ground truth\") column.\n",
    "    \n",
    "    :param output_file_1: string object which represents the name of the first output file\n",
    "    :param output_file_2: string object which represents the name of the second output file\n",
    "    :param columns: a list of string objects containg the name of the common columns\n",
    "    :return: a dataframe\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(output_file_1)\n",
    "    df2 = pd.read_csv(output_file_2)\n",
    "\n",
    "    df1 = df1[columns]\n",
    "    df2 = df2[columns]\n",
    "\n",
    "    frames = [df1, df2]\n",
    "    \n",
    "    return pd.concat(frames, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_statement_for_creating_new_table(column_datatypes, tmp_table_name):\n",
    "    \"\"\"\n",
    "    Create a general statement for creating a table in a database.\n",
    "\n",
    "    :param column_datatypes: list of string objects, where the strings represents the datatypes\n",
    "                             of each column\n",
    "    :param tmp_table_name: string containing the name of the new table\n",
    "    :return: a string object which represents the SQL statement.\n",
    "    \"\"\"\n",
    "    create_table_sql_statement = \"CREATE TABLE \" + tmp_table_name + \" (company_id SERIAL PRIMARY KEY\"\n",
    "\n",
    "    for k, v in column_datatypes.items():\n",
    "        create_table_sql_statement += \",\"\n",
    "        create_table_sql_statement += k\n",
    "        create_table_sql_statement += \" \"\n",
    "        create_table_sql_statement += v\n",
    "\n",
    "    return create_table_sql_statement + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_statement_for_copying_values_from_file(columns, tmp_table_name):\n",
    "    \"\"\"\n",
    "    Create a general statement for copying values from a csv file in a specific table.\n",
    "\n",
    "    :param columns: list of string objects, where the strings are names of columns\n",
    "    :param tmp_table_name: string containing the name of the table where values will\n",
    "                         be inserted (copied) \n",
    "    :return: a string object which represents the SQL statement\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_copy_statement = \"COPY \" + tmp_table_name + \" (\"\n",
    "    \n",
    "    for c in columns:\n",
    "        sql_copy_statement += c + \", \"\n",
    "    \n",
    "    return sql_copy_statement[:len(sql_copy_statement) - 2] + \") FROM STDIN CSV HEADER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_and_their_datatypes(result_df):\n",
    "    \"\"\"\n",
    "    Convert the dataframe's datatypes into postgres datatypes.\n",
    "    \n",
    "    :param result_df: pandas dataframe. \n",
    "    :return: a dictionary object where the keys are the columns and the values represents their datatypes.\n",
    "    \"\"\"\n",
    "    \n",
    "    column_datatypes = {}\n",
    "\n",
    "    for k, v in dict(result_df.dtypes).items():\n",
    "        if v == 'int64':\n",
    "            column_datatypes[k] = 'INT'\n",
    "        elif v == 'float64':\n",
    "            column_datatypes[k] = 'FLOAT'\n",
    "        else:\n",
    "            column_datatypes[k] = 'VARCHAR(500)'\n",
    "    \n",
    "    return column_datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateDuplicates(found_dupes, true_dupes):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall.\n",
    "    \n",
    "    :param found_dupes: result from 'dupePairs' function (the result after we apply the algorithm)\n",
    "    :param true_dupes: result from 'dupePairs' function (the ground truth)\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = found_dupes.intersection(true_dupes)\n",
    "    false_positives = found_dupes.difference(true_dupes)\n",
    "\n",
    "    if len(found_dupes) == 0:\n",
    "        precision = None\n",
    "    else:\n",
    "        precision = 1 - len(false_positives) / float(len(found_dupes))\n",
    "\n",
    "    if len(true_dupes) == 0:\n",
    "        recall = None\n",
    "    else:\n",
    "        recall = len(true_positives) / float(len(true_dupes))\n",
    "\n",
    "    logging.info('precision {}'.format(precision))\n",
    "\n",
    "    logging.info('recall {}'.format(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupePairs(colname_id, colname_index, table_name):\n",
    "    \"\"\"\n",
    "    This function creates a set which contains pairs (tuples) of numbers.\n",
    "    The pair is an ordered tuple containing two company ids. The tuple contains records that were put in\n",
    "    the same cluster.\n",
    "    \n",
    "    A company id is the primary key of a table, where we have all the companies, thus, this company id\n",
    "    is unique in that table.\n",
    "    \n",
    "    When we do the evaluation, we extract the company ids by querying the cluster_id column to find the\n",
    "    clusters that Dedupe made, and, with another call to this function we extract the company ids by\n",
    "    querying the \"label\" column of the companies to get the true clusters (the ground truth). Then,\n",
    "    the 'evaluateDuplicates' function receives these two sets of tuples and compares them to calculate\n",
    "    precision and recall\n",
    " \n",
    "    :param colname_id: string object containing the name of the column which represents the label\n",
    "                       of the examples (e.g cluster id or id) \n",
    "    :param colname_index: string object which represents the record id (the PK from the table)\n",
    "    :param table_name: string object which represents the name of the table from where we extract\n",
    "                       the examples \n",
    "    :return: a set which contains pairs (tuples) of number\n",
    "    \"\"\"\n",
    "    \n",
    "    colname_id = colname_id.split()[0]\n",
    "    colname_index = colname_index.split()[0]\n",
    "    table_name = table_name.split()[0]\n",
    "\n",
    "    dupe_d = collections.defaultdict(list)\n",
    "\n",
    "    db_cursor.execute(\"SELECT \" + colname_id + \" FROM \" + table_name)\n",
    "    rows_id = np.array(db_cursor.fetchall())\n",
    "\n",
    "    db_cursor.execute(\"SELECT \" + colname_index + \" FROM \" + table_name)\n",
    "    rows_company = np.array(db_cursor.fetchall())\n",
    "\n",
    "    for row_id, company_id in zip(rows_id, rows_company):\n",
    "        dupe_d[row_id[colname_id]].append(company_id[colname_index])\n",
    "\n",
    "    dupe_s = set([])\n",
    "    for (unique_id, cluster) in dupe_d.items():\n",
    "        if len(cluster) > 1:\n",
    "            for pair in itertools.combinations(cluster, 2):\n",
    "                dupe_s.add(frozenset(pair))\n",
    "\n",
    "    return dupe_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_file_data.get('evaluation'):\n",
    "    # evaluation parameters\n",
    "    # precision and recall is done with a database\n",
    "    db_connection = psycopg2.connect(\n",
    "        database=config_file_data['database_config'].get('database_name'),\n",
    "        user=config_file_data['database_config'].get('username'),\n",
    "        password=config_file_data['database_config'].get('password'),\n",
    "        host=config_file_data['database_config'].get('host'),\n",
    "        port=config_file_data['database_config'].get('port'),\n",
    "        cursor_factory=psycopg2.extras.RealDictCursor\n",
    "    )\n",
    "    db_cursor = db_connection.cursor()\n",
    "\n",
    "    # this is the name of the column based on which we can create correct clusters (the label)\n",
    "    # for example if the column is a unique id, then we can see which companies match based on it\n",
    "    # N.B.: This only makes sense if you will NOT use this column in the training process, i.e.,\n",
    "    # don't give it as a training field!\n",
    "    label_column_name = config_file_data['evaluation'].get('label_column_name')\n",
    "    tmp_table_name = \"tmp_test_table\"\n",
    "    columns = ['cluster_id', label_column_name]\n",
    "\n",
    "    result_df = get_merged_dataframe_containing_only_cluster_id_and_label_column(output_file_1, output_file_2, columns)\n",
    "\n",
    "    # write the dataframe as a csv looking file into a string and explicitly make the file pointer point at \n",
    "    # the beginning of the file\n",
    "    s_buf = StringIO()\n",
    "    result_df.to_csv(s_buf, index=False)\n",
    "    s_buf.seek(0)\n",
    "\n",
    "    # convert the dataframe datatypes into postgres datatypes\n",
    "    column_datatypes = get_columns_and_their_datatypes(result_df)\n",
    "\n",
    "    logging.info('importing raw data from csv...')\n",
    "    db_cursor.execute(\"DROP TABLE IF EXISTS \" + tmp_table_name)\n",
    "\n",
    "    db_cursor.execute(sql_statement_for_creating_new_table(column_datatypes, tmp_table_name))\n",
    "    db_connection.commit()\n",
    "\n",
    "    db_cursor.copy_expert(sql_statement_for_copying_values_from_file(columns, tmp_table_name), s_buf)\n",
    "    db_connection.commit()\n",
    "\n",
    "    logging.info('generating the true and test clusters...')\n",
    "    true_dupes = dupePairs(label_column_name, 'company_id', tmp_table_name)\n",
    "    test_dupes = dupePairs('cluster_id', 'company_id', tmp_table_name)\n",
    "    \n",
    "    # True dupes represents a list containing the real clusters which are made using the label column.(for example,\n",
    "    # the id column)\n",
    "    # Found dupes represents a list containing the clusters found by the library\n",
    "    logging.info(\"True dupes: {}\".format(len(true_dupes)))\n",
    "    logging.info(\"Found dupes: {}\".format(len(test_dupes)))\n",
    "    evaluateDuplicates(test_dupes, true_dupes)\n",
    "\n",
    "    db_cursor.close()\n",
    "    db_connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
