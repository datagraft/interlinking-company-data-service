{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import itertools\n",
    "import pickle\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import dedupe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "\n",
    "from io import StringIO\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_file_name = 'configuration_file_dedupe.json'\n",
    "config_file_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configuration_file_name, 'r') as config_file:\n",
    "    config_file_data = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "input_file_1 = config_file_data.get('input_file_1')\n",
    "input_file_2 = config_file_data.get('input_file_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    This function does a little bit of data cleaning with the help of Unidecode and Regex libraries.\n",
    "    Things like casing, extra spaces and new lines can be ignored.\n",
    "    \n",
    "    :param column: string object which represents a cell from the csv file\n",
    "    :return: the preprocessed column\n",
    "    \"\"\"\n",
    "    \n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    \n",
    "    if not column :\n",
    "        column = None\n",
    "        \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    This function reads CSV file and creates a dictionary of records, \n",
    "    where the key is a unique record ID (name of the file + index).\n",
    "    \n",
    "    :param filename: string object which represents the name of the\n",
    "                     input file\n",
    "    :return: a dictionary object containing all the rows read from the CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    \n",
    "    partial_key = filename\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        \n",
    "        # delete the first 4 characters of the partial key if they are \"tmp_\"\n",
    "        if partial_key.find(\"tmp_\") == 0:\n",
    "            partial_key = partial_key[4:]\n",
    "            \n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k,v) in row.items()])\n",
    "            data_d[partial_key + str(i)] = dict(clean_row)\n",
    "            \n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_containing_only_the_common_fields_in_both_datasets(input_file, common_fields):\n",
    "    \"\"\"\n",
    "    Read our data from a CSV file only the common fields of the both datasets.\n",
    "    \n",
    "    Input: 'input_file'    - string object which represents the name of the\n",
    "                             input file\n",
    "           'common_fields' - a list of string objects containing the name of the \n",
    "                             common fields of the both datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    #create a temporary file with only common columns\n",
    "    tmp_input_file_with_common_cols = \"tmp_\" + input_file\n",
    "    \n",
    "    df = pd.read_csv(input_file, dtype = object)\n",
    "    df = df[common_fields]\n",
    "    \n",
    "    df.to_csv(tmp_input_file_with_common_cols, index = False)\n",
    "    \n",
    "    #read the data from this temporary file\n",
    "    data_d = read_data(tmp_input_file_with_common_cols)\n",
    "    \n",
    "    #remove the temporary file \n",
    "    os.remove(tmp_input_file_with_common_cols)\n",
    "    \n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_fields_of_the_datasets_or_the_given_fields(config_file_data, input_file_1, input_file_2):\n",
    "    \"\"\"\n",
    "    Get the common columns (fields) of both datasets if the user did not specify \n",
    "    them in the configuration file.\n",
    "    \n",
    "    Input: 'config_file_data' - a dictionary containing the information of the\n",
    "                                configuration file\n",
    "           'input_file_1' - string object which represents the name of the first dataset\n",
    "           'input_file_2' - string object which represents the name of the second dataset\n",
    "    \"\"\"\n",
    "    given_fields = []\n",
    "    \n",
    "    # if some fields were specified, get them in a list\n",
    "    if config_file_data['training'].get('field_definitions'):\n",
    "        field_definitions = config_file_data['training'].get('field_definitions')\n",
    "        \n",
    "        for f in field_definitions:\n",
    "            given_fields.append(f['field'])  \n",
    "        \n",
    "    # get the common columns of both datasets\n",
    "    df1 = pd.read_csv(input_file_1, dtype = object)\n",
    "    df2 = pd.read_csv(input_file_2, dtype = object)\n",
    "      \n",
    "    # postgres only has lower case column names --> make lower case the dataframe column names    \n",
    "    f1_header_columns = set([x.lower() for x in list(df1.columns.values)])\n",
    "    f2_header_columns = set([x.lower() for x in list(df2.columns.values)])\n",
    "    \n",
    "    common_cols_from_datasets = list(f1_header_columns.intersection(f2_header_columns))\n",
    "    \n",
    "    # make sure that the given fields (if given) can be found in the common columns of the datasets\n",
    "    if len(given_fields) > 0:\n",
    "        return list(set(given_fields).intersection(set(common_cols_from_datasets)))\n",
    "    \n",
    "    return common_cols_from_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = get_common_fields_of_the_datasets_or_the_given_fields(config_file_data, input_file_1, input_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading records from new_first_dataset_1000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:1000 records read\n"
     ]
    }
   ],
   "source": [
    "logging.info('reading records from {}'.format(input_file_1))\n",
    "first_dataset = read_dataset_containing_only_the_common_fields_in_both_datasets(input_file_1, common_columns)\n",
    "logging.info('{} records read'.format(len(first_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading records from new_second_dataset_1000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:1000 records read\n"
     ]
    }
   ],
   "source": [
    "logging.info('reading records from {}'.format(input_file_2))\n",
    "second_dataset = read_dataset_containing_only_the_common_fields_in_both_datasets(input_file_2, common_columns)\n",
    "logging.info('{} records read'.format(len(second_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:starting training..\n"
     ]
    }
   ],
   "source": [
    "logging.info('starting training..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_fields(config_file_data, common_columns):\n",
    "    \"\"\"\n",
    "    Define the common columns (fields) the library will pay attention to \n",
    "    by creating a list of dictionaries where each dictionary's keys will be 'field' and\n",
    "    'type'.\n",
    "    E.g.: [{'field' : 'field1', 'type' : 'String'}, {'field' : 'field2', 'type' : 'String'}]\n",
    "    For more information about the accepted types, read more here:\n",
    "    https://docs.dedupe.io/en/latest//Variable-definition.html\n",
    "    \n",
    "    Input: 'config_file_data' - dictionary containg the the information of the\n",
    "                                configuration file\n",
    "           'common_columns' - a list of string objects containing the name of the \n",
    "                             common columns(fields) of the both datasets\n",
    "    \"\"\"\n",
    "    if config_file_data['training'].get('field_definitions'):\n",
    "        fields = []\n",
    "        for c in common_columns:\n",
    "            for f in config_file_data['training']['field_definitions']:\n",
    "                if f['field'] == c:\n",
    "                    fields.append({'field' : c, 'type' : f['type']})\n",
    "                    break\n",
    "                    \n",
    "        return fields\n",
    "    \n",
    "    #if the fields are not specified in configuration file, then create\n",
    "    #a list of dictionaries where the type of the field will be \"String\" \n",
    "    #by default\n",
    "    fields = [{'field' : f, 'type' : 'String'} for f in common_columns]\n",
    "    \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertain_pairs(deduper, nr_uncertain_pairs):\n",
    "    \"\"\"\n",
    "    This function gets all the uncertain pairs by using the library's function uncertainPairs.\n",
    "    \n",
    "    :param deduper: the library object\n",
    "    :param nr_uncertain_pairs: how many uncertain pairs the library should give to the user to label\n",
    "    :return: a list of tuples where every tuple represents a pair of examples which library is uncertain to label them.\n",
    "    \"\"\"\n",
    "    \n",
    "    uncertain_pairs = [] \n",
    "    \n",
    "    for i in range(0, nr_uncertain_pairs):\n",
    "        uncertain_pair = deduper.uncertainPairs()\n",
    "        uncertain_pairs.append(uncertain_pair.pop())\n",
    "    \n",
    "    return uncertain_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the algorithm is the training part. In this part it is used the [RecordLink](https://docs.dedupe.io/en/latest/API-documentation.html#RecordLink) class from dedupe. This class is used because the algorithm takes two input files and try to find one-to-one match between different examples from both datasets.\n",
    "A RecordLink object is created using the common fields of the both datasets. If the user specifies these fields in the configuration file then these columns will be used (they may not be all the common columns). These fields are used as features for the classifier. In this part the algorithm learns different weights for these fields and then builds a model. Basically, the model is represented by the the fields and their weights.\n",
    "You can notice that here we have three cases:\n",
    "* In the first case it doesn't exist a training and a settings file so the user should create them. First the algorithm samples some examples to do a mini training used for giving to the user the examples that the library is uncertain about. The user should label this examples by typing y(yes) in case the user considers that these examples match, n(no) in case the examples don't match, u(unsure) if the user is unsure to label, p(previous) if the user figures out that he/she don't label correctly the previous example or f(finish) to finish the labeling part. At the end of this process, which is called active learning, a training file is created and the library uses it to continue the training. The result is a model which is saved in a settings file. For building the model, the library uses [Logistic Regression](https://github.com/dedupeio/rlr) [classifier](https://docs.dedupe.io/en/latest/API-documentation.html#Dedupe.classifier).\n",
    "* In the second case a training file was created. Then the labeling part is skipped and a new model is created.\n",
    "* In the third case a settings file was created and the labeling part and training part is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if 'create_training_file_by_client' parameter is true this cell will create the 'uncertain_pairs_file'\n",
    "# configuration parameters used for the training part\n",
    "nr_examples_sampled_for_training = config_file_data['training'].get('nr_of_examples_for_training')\n",
    "training_fields = get_training_fields(config_file_data, common_columns)\n",
    "create_training_file = config_file_data['training']['create_training_file_by_client']\n",
    "training_file = config_file_data['training'].get('training_file')\n",
    "settings_file = config_file_data['training'].get('settings_file')\n",
    "\n",
    "if settings_file:\n",
    "    logging.info('reading from {}'.format(settings_file))\n",
    "    with open(settings_file, 'rb') as sf :\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "        \n",
    "else:\n",
    "    linker = dedupe.RecordLink(training_fields)\n",
    "\n",
    "    linker.sample(first_dataset, second_dataset, nr_examples_sampled_for_training)\n",
    "     \n",
    "    # if the user wants to create a training file on the client side we will make available\n",
    "    # through a GET request a binary file containing 200 pairs of examples \n",
    "    # that Dedupe doesn't know if they match or not; then, on the client side\n",
    "    # the user will label the pairs (match or distinct) and the client will\n",
    "    # make a POST request with the newly created training file\n",
    "    if create_training_file:\n",
    "        uncertain_pairs = get_uncertain_pairs(linker, 200)\n",
    "        with open(\"uncertain_pairs_file\", \"wb\") as f:\n",
    "            pickle.dump(uncertain_pairs, f)\n",
    "\n",
    "    else:     \n",
    "        if training_file:\n",
    "            logging.info('reading labeled examples from {}'.format(training_file))\n",
    "            with open(training_file) as tf :\n",
    "                linker.readTraining(tf)\n",
    "        else:    \n",
    "            logging.info('starting active labeling...')\n",
    "            dedupe.consoleLabel(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        # if the training and settings files were not specified, but you want to keep them between runs, \n",
    "        # rename them or save them somewhere else, because they will be overwritten every time the algorithm is run\n",
    "        if not training_file:\n",
    "            with open(\"training_file.json\", 'w') as tf :\n",
    "                linker.writeTraining(tf)\n",
    "\n",
    "        if not settings_file:\n",
    "            with open(\"settings_file\", 'wb') as sf :\n",
    "                linker.writeSettings(sf)\n",
    "\n",
    "        linker.cleanupTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings_file is None and create_training_file:\n",
    "    logging.info('reading labeled examples from the newly created training file {}'.format(training_file))\n",
    "    \n",
    "    with open(training_file) as tf :\n",
    "        linker.readTraining(tf)\n",
    "\n",
    "    linker.train()\n",
    "    \n",
    "    with open(\"settings_file\", 'wb') as sf :\n",
    "        linker.writeSettings(sf)\n",
    "\n",
    "    linker.cleanupTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters used for the 'threshold' method from Dedupe\n",
    "threshold_value = None\n",
    "\n",
    "if config_file_data.get('threshold') or config_file_data.get('compute_threshold'):\n",
    "    if config_file_data.get('threshold'):\n",
    "        threshold_value = config_file_data.get('threshold')\n",
    "    else:\n",
    "        logging.info('find the best threshold...')\n",
    "\n",
    "        recall_weight = config_file_data['compute_threshold'].get('recall_weight')\n",
    "\n",
    "        sample_nr_of_examples_for_threshold = \\\n",
    "                config_file_data['compute_threshold'].get('nr_of_sample_data_for_threshold')\n",
    "\n",
    "        # get n examples from the input dataset, where n = 'sample_nr_of_examples_for_threshold' \n",
    "        sample_data_threshold_first_dataset = {\n",
    "            k: first_dataset[k] for k in list(first_dataset)[:int(sample_nr_of_examples_for_threshold)]\n",
    "        }\n",
    "        sample_data_threshold_second_dataset = {\n",
    "            k: second_dataset[k] for k in list(second_dataset)[:int(sample_nr_of_examples_for_threshold)]\n",
    "        }\n",
    "\n",
    "        threshold_value = linker.threshold(sample_data_threshold_first_dataset, \n",
    "                                            sample_data_threshold_second_dataset, \n",
    "                                           recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info('clustering...')\n",
    "print(threshold_value)\n",
    "\n",
    "if threshold_value:\n",
    "    linked_records = linker.match(first_dataset, second_dataset, threshold_value)\n",
    "else:\n",
    "    linked_records = linker.match(first_dataset, second_dataset)\n",
    "\n",
    "logging.info('# duplicate sets {}'.format(len(linked_records)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maximum value for cluster_id found in the database\n",
    "# we will create new clusters, which will have ids starting from this value onward\n",
    "cluster_id = int(config_file_data.get('last_cluster_id')) if config_file_data.get('last_cluster_id') else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership = {}\n",
    "\n",
    "#linked_records is a list which contains a tuple of record ids\n",
    "#of the examples which match and their score\n",
    "\n",
    "#create a dictionary where the keys will be the record id given\n",
    "#in the read_data method and the values will be tuples which\n",
    "#contain the cluster id and the score\n",
    "for cluster, score in linked_records:\n",
    "    cluster_id += 1    \n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score) \n",
    "\n",
    "unique_id = cluster_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_file(filename, output_file):\n",
    "    \"\"\"\n",
    "    Create an output file which contains cluster id and the link score columns\n",
    "    besides the initial columns from the input file \n",
    "    \n",
    "    Input: 'filename' - string object which represents the name of one dataset\n",
    "           'output_file' - string object which represents the name of the output\n",
    "                           file \n",
    "    \"\"\"\n",
    "    \n",
    "    global unique_id\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        with open(filename) as f_input :\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            heading_row = next(reader)\n",
    "            heading_row.insert(0, 'link_score')\n",
    "            heading_row.insert(0, 'cluster_id')\n",
    "            writer.writerow(heading_row)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                \n",
    "                #the examples which have not a match with other examples will be put\n",
    "                #in their own cluster\n",
    "                if cluster_details is None:\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else:\n",
    "                    cluster_id, score = cluster_details\n",
    "                    \n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"output_\" + input_file_1\n",
    "output_file_2 = \"output_\" + input_file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('create output files...')\n",
    "create_output_file(input_file_1, output_file_1)\n",
    "create_output_file(input_file_2, output_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('starting evaluation...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_dataframe_containing_only_cluster_id_and_label_column(output_file_1, output_file_2, columns):\n",
    "    \"\"\"\n",
    "    Create a merged dataframe which contains only cluster id and label column\n",
    "    \n",
    "    Input: 'output_file_1' - string object which represents the name of the first output file\n",
    "           'output_file_2' - string object which represents the name of the second output file\n",
    "           'columns' - a list of string objects containg the name of the common columns\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(output_file_1)\n",
    "    df2 = pd.read_csv(output_file_2)\n",
    "\n",
    "    df1 = df1[columns]\n",
    "    df2 = df2[columns]\n",
    "\n",
    "    frames = [df1, df2]\n",
    "    \n",
    "    return pd.concat(frames, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_statement_for_creating_new_table(column_datatypes, tmp_table_name):\n",
    "    \"\"\"\n",
    "    Create a general statement for creating a table in a database.\n",
    "    \n",
    "    Input: 'column_datatypes' - list of string objects, where the strings represents the datatypes\n",
    "                               of each column\n",
    "           'tmp_table_name' - string containing the name of the new table\n",
    "    \"\"\"\n",
    "    create_table_sql_statement = \"CREATE TABLE \" + tmp_table_name + \" (company_id SERIAL PRIMARY KEY\"\n",
    "\n",
    "    for k,v in column_datatypes.items():\n",
    "        create_table_sql_statement += \",\"\n",
    "        create_table_sql_statement += k\n",
    "        create_table_sql_statement += \" \"\n",
    "        create_table_sql_statement += v\n",
    "\n",
    "    return create_table_sql_statement + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_statement_for_copying_values_from_file(columns, tmp_table_name):\n",
    "    \"\"\"\n",
    "    Create a general statement for copying values from a csv file in a specific table.\n",
    "    \n",
    "    Input: 'columns' - list of string objects, where the strings are names of columns\n",
    "           'tmp_table_name' - string containing the name of the table where values will\n",
    "                         be inserted (copied)    \n",
    "    \"\"\"\n",
    "    \n",
    "    sql_copy_statement = \"COPY \" + tmp_table_name + \" (\"\n",
    "    \n",
    "    for c in columns:\n",
    "        sql_copy_statement += c + \", \"\n",
    "    \n",
    "    return sql_copy_statement[:len(sql_copy_statement) - 2] + \") FROM STDIN CSV HEADER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_and_their_datatypes(result_df):\n",
    "    \"\"\"\n",
    "    Convert the dataframe's datatypes into postgres datatypes\n",
    "    \n",
    "    Input: 'result_df' - pandas dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    column_datatypes = {}\n",
    "\n",
    "    for k,v in dict(result_df.dtypes).items():\n",
    "        if v == 'int64':\n",
    "            column_datatypes[k] = 'INT'\n",
    "        elif v == 'float64':\n",
    "            column_datatypes[k] = 'FLOAT'\n",
    "        else:\n",
    "            column_datatypes[k] = 'VARCHAR(500)'\n",
    "    \n",
    "    return column_datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateDuplicates(found_dupes, true_dupes):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall.\n",
    "    \n",
    "    Input: 'found_dupes' - result from 'dupePairs' function (the result after we apply the algorithm)\n",
    "                          \n",
    "           'true_dupes' - result from 'dupePairs' function (the ground truth)                          \n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = found_dupes.intersection(true_dupes)\n",
    "    false_positives = found_dupes.difference(true_dupes)\n",
    "    \n",
    "    if len(found_dupes) == 0:\n",
    "        precision = None\n",
    "    else:\n",
    "        precision = 1 - len(false_positives) / float(len(found_dupes))\n",
    "        \n",
    "    if len(true_dupes) == 0:\n",
    "        recall = None\n",
    "    else:\n",
    "        recall = len(true_positives) / float(len(true_dupes))\n",
    "        \n",
    "\n",
    "    logging.info('precision {}'.format(precision))\n",
    "\n",
    "    logging.info('recall {}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupePairs(colname_id, colname_index, table_name):\n",
    "    \"\"\"\n",
    "    Returns a set which contains pairs (tuples) of numbers.\n",
    "    The pair is an ordered tuple containing two record_ids. The tuple contains records that were put in\n",
    "    the same cluster.\n",
    "    \n",
    "    A record id is the primary key of a table, where we have all the companies, thus, this record id\n",
    "    is unique in that table.\n",
    "    \n",
    "    When we do the evaluation, we extract the record ids by querying the cluster_id column to find the\n",
    "    clusters that Dedupe made, and, with another call to this function we extract the record ids by\n",
    "    querying the official ids of the companies to get the true clusters (the ground truth). Then,\n",
    "    the 'evaluateDuplicates' function receives these two sets of tuples and compares them to calculate\n",
    "    precision and recall\n",
    "    \n",
    "    Input: 'colname_id' - string object containg the name of the column which represents the label\n",
    "                          of the examples (e.g cluster id or id)\n",
    "           'colname_index' - string object which represents the record id (the PK from the table)\n",
    "           'table_name' - string object which represents the name of the table from where we extract\n",
    "                          the examples\n",
    "    \"\"\"\n",
    "    # TODO better comments\n",
    "    colname_id = colname_id.split()[0]\n",
    "    colname_index = colname_index.split()[0]\n",
    "    table_name = table_name.split()[0]\n",
    "    \n",
    "    dupe_d = collections.defaultdict(list)\n",
    "    \n",
    "    db_cursor.execute(\"SELECT \" + colname_id + \" FROM \" + table_name)\n",
    "    rows_id = np.array(db_cursor.fetchall())\n",
    "    \n",
    "    db_cursor.execute(\"SELECT \" + colname_index + \" FROM \" + table_name)\n",
    "    rows_company = np.array(db_cursor.fetchall())\n",
    "    \n",
    "    for row_id, company_id in zip(rows_id, rows_company):\n",
    "        dupe_d[row_id[colname_id]].append(company_id[colname_index])\n",
    "    \n",
    "    \n",
    "    dupe_s = set([])\n",
    "    for (unique_id, cluster) in dupe_d.items():\n",
    "        if len(cluster) > 1:\n",
    "            for pair in itertools.combinations(cluster, 2):\n",
    "                dupe_s.add(frozenset(pair))\n",
    "\n",
    "    return dupe_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_file_data.get('evaluation'):\n",
    "    # evaluation parameters\n",
    "    # precision and recall is done with a database\n",
    "    db_connection = psycopg2.connect(\n",
    "        database = config_file_data['database_config'].get('database_name'),\n",
    "        user =     config_file_data['database_config'].get('username'),\n",
    "        password = config_file_data['database_config'].get('password'),\n",
    "        host =     config_file_data['database_config'].get('host'),\n",
    "        port =     config_file_data['database_config'].get('port'),\n",
    "        cursor_factory = psycopg2.extras.RealDictCursor\n",
    "    )\n",
    "    db_cursor = db_connection.cursor()\n",
    "\n",
    "    # this is the name of the column based on which we can create correct clusters (the label)\n",
    "    # for example if the column is a unique id, then we can see which companies match based on it\n",
    "    # N.B.: This only makes sense if you will NOT use this column in the training process, i.e.,\n",
    "    # don't give it as a training field!\n",
    "    label_column_name = config_file_data['evaluation'].get('label_column_name')\n",
    "    tmp_table_name = \"tmp_test_table\"\n",
    "    columns = ['cluster_id', label_column_name]\n",
    "\n",
    "    result_df = get_merged_dataframe_containing_only_cluster_id_and_label_column(output_file_1, output_file_2,\\\n",
    "                                                                                  columns)\n",
    "\n",
    "    # write the dataframe as a csv looking file into a string and explicitly make the file pointer point at \n",
    "    # the beginning of the file\n",
    "    s_buf = StringIO()\n",
    "    result_df.to_csv(s_buf, index = False)\n",
    "    s_buf.seek(0)\n",
    "\n",
    "    # convert the dataframe datatypes into postgres datatypes\n",
    "    column_datatypes = get_columns_and_their_datatypes(result_df)\n",
    "\n",
    "    logging.info('importing raw data from csv...')\n",
    "    db_cursor.execute(\"DROP TABLE IF EXISTS \" + tmp_table_name)\n",
    "\n",
    "    db_cursor.execute(sql_statement_for_creating_new_table(column_datatypes, tmp_table_name))\n",
    "    db_connection.commit()\n",
    "\n",
    "    db_cursor.copy_expert(sql_statement_for_copying_values_from_file(columns, tmp_table_name), s_buf)\n",
    "    db_connection.commit()\n",
    "    \n",
    "    logging.info('generating the true and test clusters...')\n",
    "    true_dupes = dupePairs(label_column_name, 'company_id', tmp_table_name)\n",
    "    test_dupes = dupePairs('cluster_id', 'company_id', tmp_table_name)\n",
    "\n",
    "    logging.info(\"True dupes: {}\".format(len(true_dupes)))\n",
    "    logging.info(\"Found dupes: {}\".format(len(test_dupes)))\n",
    "    evaluateDuplicates(test_dupes, true_dupes)\n",
    "\n",
    "    db_cursor.close()\n",
    "    db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
